<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ML Model Monitoring Metrics | Matthew Burke’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="ML Model Monitoring Metrics" />
<meta name="author" content="Matthew" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Detecting silent model failures early and consistently" />
<meta property="og:description" content="Detecting silent model failures early and consistently" />
<link rel="canonical" href="http://localhost:4000/data%20science/2024/07/28/model-monitoring-metrics.html" />
<meta property="og:url" content="http://localhost:4000/data%20science/2024/07/28/model-monitoring-metrics.html" />
<meta property="og:site_name" content="Matthew Burke’s Blog" />
<meta property="og:image" content="http://localhost:4000/images/placeholder.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-28T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"Detecting silent model failures early and consistently","author":{"@type":"Person","name":"Matthew"},"@type":"BlogPosting","url":"http://localhost:4000/data%20science/2024/07/28/model-monitoring-metrics.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/images/generative-art-31.png"},"name":"Matthew"},"image":{"thumbnail":"/images/placeholder.png","url":"http://localhost:4000/images/placeholder.png","@type":"imageObject"},"headline":"ML Model Monitoring Metrics","dateModified":"2024-07-28T00:00:00-07:00","datePublished":"2024-07-28T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/data%20science/2024/07/28/model-monitoring-metrics.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Matthew Burke&#39;s Blog" href="/atom.xml">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#31ac94">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#31ac94">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-50J88PJLNB"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-50J88PJLNB');
</script>

</head>


  <body class="layout--post  ml-model-monitoring-metrics">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/search/">Search</a></li><li><a href="/about">About</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="Matthew Burke's Blog">
        <img src="/images/generative-art-31.png" class="site-logo-img animated fadeInDown" alt="Matthew Burke's Blog">
      </a>
    
    
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    
  
  
  

  <div class="page-image">
    <img src="/images/placeholder.png" class="entry-feature-image u-photo" alt="ML Model Monitoring Metrics" >
    
  </div>


    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">ML Model Monitoring Metrics
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><div class="author-info">

<span class="read-time">6 min read</span>

    <time class="page-date dt-published" datetime="2024-07-28T00:00:00-07:00"><a class="u-url" href="">July 28, 2024</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#data-science" title="Pages filed under Data Science">Data Science</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/tags/#python" title="Pages tagged python" rel="tag">python</a></li><li class="page-taxonomy"><a href="/tags/#ml" title="Pages tagged ml" rel="tag">ml</a></li><li class="page-taxonomy"><a href="/tags/#metrics" title="Pages tagged metrics" rel="tag">metrics</a></li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <h2 id="ml-model-monitoring-metrics-a-hierarchical-approach-to-mitigating-silent-failures"><strong>ML Model Monitoring Metrics: A Hierarchical Approach to Mitigating Silent Failures</strong></h2>

<p>In the ever-evolving landscape of machine learning, maintaining a fleet of hundreds to thousands of production models can be a daunting task. These models are susceptible to a wide array of silent errors that can go undetected for extended periods. The consequences of such errors can be significant (and expensive).</p>

<p>Model monitoring serves as a crucial defense mechanism against these hidden threats. However, monitoring encompasses a broad spectrum of concerns, from model health to deployment health. In this article, we will focus specifically on model health, which refers to the model’s ability to perform as expected in the production environment, rather than identifying issues within the deployment infrastructure.</p>

<h3 id="the-hierarchy-of-model-monitoring-metrics"><strong>The Hierarchy of Model Monitoring Metrics</strong></h3>

<p>To effectively monitor model health, I generally categorize metrics into three tiers: primary, secondary, and tertiary. They roughly correlate, not to importance per se, but in order of urgency and complexity. Primary metrics are a need to have, secondary are great, and tertiary are valuable in catching other less common failure types not covered by the first two.</p>

<h4 id="primary-metrics-model-performance-as-the-source-of-truth"><strong>Primary Metrics: Model Performance as the Source of Truth</strong></h4>

<p>The cornerstone of model monitoring lies in the primary metrics, which directly measure the model’s performance against its intended objectives. These metrics are the gold standard, providing the most accurate assessment of the model’s efficacy, and generally correspond to precision/recall, AUROC/PRAUC or specialized metrics such as NCDG for recommender systems.</p>

<p>It is imperative to carefully define these metrics and establish a lower bound threshold that triggers a thorough review. However, relying solely on thresholds can be reactive. A more proactive approach involves trend analysis or point-in-time shift analysis. These techniques allow us to extrapolate the model’s performance trajectory, potentially identifying issues before they reach critical levels.One approach I have seen is to establish an initial threshold that suggests review, and a second threshold closer to the unacceptable failure metric that can be considered urgent and requiring action.</p>

<h5 id="why-use-other-metrics"><strong>Why Use Other Metrics?</strong></h5>

<p>In many real-world scenarios, there can be a significant lag between the time a model makes a prediction and the time the corresponding ground truth data becomes available. This delay can be attributed to various factors, such as manual feedback loops, long-term user behavior patterns, or infrastructural limitations. In such cases, relying solely on primary metrics can lead to delayed detection of model degradation.</p>

<h5 id="the-role-of-proxy-metrics"><strong>The Role of Proxy Metrics</strong></h5>

<p>To address this challenge, I want to introduce proxy metrics, which serve as substitutes for primary metrics. Proxy metrics have a shorter measurement lag and exhibit a high correlation with the target metric. One example would be using 30-day activity as an indicator of long-term (yearly or longer) engagement. Clearly, they will be related, but correlation will not be perfect due to the expansive time involved in the main metric. We choose a metric with less precision in order to be more agile with our decision making process.</p>

<p>However, it’s crucial to exercise caution when selecting and interpreting proxy metrics. The relationship between a proxy metric and the target metric can shift over time due to changes in the underlying data distribution or model behavior, and it should be reviewed on a regular basis.</p>

<h5 id="the-importance-of-segmentation"><strong>The Importance of Segmentation</strong></h5>

<p>In addition to overall performance metrics, it is often valuable to segment the data and monitor the model’s performance across different groups or cohorts. This approach can reveal performance disparities that might be masked by aggregate metrics. For instance, a model might perform well overall but exhibit suboptimal performance for a specific demographic or user segment. Identifying such disparities is crucial for ensuring equitable outcomes and maintaining trust in the model’s predictions.</p>

<h4 id="secondary-metrics-drift-and-data-quality"><strong>Secondary Metrics: Drift and Data Quality</strong></h4>

<p>Secondary metrics, including drift and data quality metrics, play a vital role in early detection of potential model issues. These metrics can be computed rapidly and provide valuable insights into the model’s health.</p>

<h5 id="data-quality-checks"><strong>Data Quality Checks</strong></h5>

<p>Data quality checks encompass a range of measures, including data freshness, null values, and out-of-range errors. While some of these checks can be automated using default values, it is often beneficial to customize them for specific features, particularly those that are critical to the model’s performance.</p>

<p>One challenge in data quality monitoring is determining how to handle errors in models with a large number of features. It is impractical to manually review every error for every feature. However, logging all data used in predictions, even after anonymization, can provide a valuable resource for post-hoc analysis and troubleshooting.</p>

<h5 id="output-drift-as-a-canary"><strong>Output Drift as a Canary</strong></h5>

<p>Output drift, which refers to changes in the distribution of the model’s predictions over time, can be a strong indicator of conceptual drift. Conceptual drift occurs when the underlying population of the model making predictions for shifts, or the relationship between the target and the input data, changes.</p>

<p>Imagine a model as a hiker trying to reach the highest point on a landscape. The model finds a peak and settles there during training. However, over time, the landscape changes. Even if the model’s performance metrics remain high, it might no longer be at the optimal peak. Output drift is like noticing that the surrounding scenery has changed, signaling that it might be time for the model to explore and find a new peak.</p>

<h5 id="input-drift"><strong>Input Drift</strong></h5>

<p>Input drift, which tracks changes in the distribution of the input data, complements output drift. It can help detect unexpected shifts caused by upstream or downstream product changes and pinpoint the source of output drift. However, input drift is often underutilized due to uncertainty about how many alerts warrant investigation.</p>

<p>Additionally, this can be seen as unnecessary if output drift is also being monitored. The main use case is in either rule-based systems, systems where the model predictions aren’t readily available, or the predictions are being made on a sub-population of entities that are filtered based on input features. Measuring input drift can alert the model maintainer to revise the thresholds used in entity selection, even if the primary model metrics are still performing within expectations.</p>

<h4 id="tertiary-metrics-checking-for-unusual-errors"><strong>Tertiary Metrics: Checking for Unusual Errors</strong></h4>

<p>Tertiary metrics focus on identifying unusual patterns or errors in the model’s predictions. These metrics can provide valuable clues about potential issues that might not be captured by primary or secondary metrics.</p>

<h5 id="spike-in-identical-predictions"><strong>Spike in Identical Predictions</strong></h5>

<p>A sudden increase in the number of predictions with the same value can be a red flag. This phenomenon might indicate an issue with an upstream data source, such as a data pipeline error or a change in the data collection process.</p>

<h5 id="shap-feature-importance-shifts"><strong>SHAP Feature Importance Shifts</strong></h5>

<p>SHAP (SHapley Additive exPlanations) feature importance, which quantifies the contribution of each feature to the model’s predictions, can also be monitored over time. Significant shifts in feature importance can indicate drift, albeit in a more subtle way than output or input drift. However, interpreting and acting upon such shifts can be challenging.</p>

<h5 id="bias-and-fairness"><strong>Bias and Fairness</strong></h5>

<p>Monitoring for bias and fairness is essential to ensure that the model’s predictions are equitable and do not discriminate against certain groups. This involves assessing the model’s performance across different demographic groups and identifying any disparities in outcomes.</p>

<h5 id="calibration"><strong>Calibration</strong></h5>

<p>Calibration metrics, such as Expected Calibration Error (ECE) or Brier Score, assess the agreement between the model’s predicted probabilities and the observed outcomes. A well-calibrated model assigns probabilities that accurately reflect the likelihood of the predicted event occurring.</p>

<h3 id="final-thoughts"><strong>Final Thoughts</strong></h3>

<p>Model monitoring is difficult in that it requires a thorough understanding of the models’ application, data availability, and stakeholders’ risk tolerance. There aren’t always clear guidelines on what metrics should generate an investigation/model retrain, but neglecting to implement monitoring due to indecision should be unacceptable to modern companies operating at scale. The cost of not doing could easily outweigh the cost of developing and maintaining proper monitoring solutions.</p>

        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/data%20science/2023/09/28/distribution-drift-tolerance.html">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> Distribution Drift Tolerance

      </span>
    </a>
  

  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://instagram.com/yot_club_"><i class="fab fa-instagram fa-2x" title="Instagram"></i></a><a class="social-icon" href="https://github.com/mwburke"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>&copy; 2024 Matthew Burke's Blog. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
