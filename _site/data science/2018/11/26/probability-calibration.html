<!DOCTYPE html>
<!--
    So Simple Jekyll Theme 3.2.0
    Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/so-simple-theme/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  

  
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Probability Calibration | Matthew Burke’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Probability Calibration" />
<meta name="author" content="Matthew Burke" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Predictions As Actual Probabilities" />
<meta property="og:description" content="Predictions As Actual Probabilities" />
<link rel="canonical" href="https://mwburke.github.io/data%20science/2018/11/26/probability-calibration.html" />
<meta property="og:url" content="https://mwburke.github.io/data%20science/2018/11/26/probability-calibration.html" />
<meta property="og:site_name" content="Matthew Burke’s Blog" />
<meta property="og:image" content="https://mwburke.github.io/images/placeholder.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-26T00:00:00-08:00" />
<script type="application/ld+json">
{"description":"Predictions As Actual Probabilities","author":{"@type":"Person","name":"Matthew Burke"},"@type":"BlogPosting","url":"https://mwburke.github.io/data%20science/2018/11/26/probability-calibration.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://mwburke.github.io/images/generative-art-31.png"},"name":"Matthew Burke"},"image":{"thumbnail":"/images/calibration_curve_2.png","url":"https://mwburke.github.io/images/placeholder.png","@type":"imageObject"},"headline":"Probability Calibration","dateModified":"2018-11-26T00:00:00-08:00","datePublished":"2018-11-26T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mwburke.github.io/data%20science/2018/11/26/probability-calibration.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/skins/light.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,700,700i|Lora:400,400i,700,700i">
  <link rel="alternate" type="application/atom+xml" title="Matthew Burke&#39;s Blog" href="/atom.xml">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#31ac94">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="theme-color" content="#31ac94">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-50J88PJLNB"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-50J88PJLNB');
</script>

</head>


  <body class="layout--post  probability-calibration">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    
  <div class="navigation-wrapper">
    <a href="#menu-toggle" id="menu-toggle">Menu</a>
    <nav id="primary-nav" class="site-nav animated drop">
      <ul><li><a href="/posts/">Posts</a></li><li><a href="/categories/">Categories</a></li><li><a href="/tags/">Tags</a></li><li><a href="/search/">Search</a></li><li><a href="/about">About</a></li></ul>
    </nav>
  </div><!-- /.navigation-wrapper -->


    <header class="masthead">
  <div class="wrap">
    
      <a href="/" class="site-logo" rel="home" title="Matthew Burke's Blog">
        <img src="/images/generative-art-31.png" class="site-logo-img animated fadeInDown" alt="Matthew Burke's Blog">
      </a>
    
    
    
  </div>
</header><!-- /.masthead -->


    <main id="main" class="main-content" aria-label="Content">
  <article class="h-entry">
    
  
  
  

  <div class="page-image">
    <img src="/images/placeholder.png" class="entry-feature-image u-photo" alt="Probability Calibration" >
    
  </div>


    <div class="page-wrapper">
      <header class="page-header">
        
        
          <h1 id="page-title" class="page-title p-name">Probability Calibration
</h1>
        
      </header>

      <div class="page-sidebar">
        <div class="page-author h-card p-author"><img src="/images/matthew-burke-photo.jpg" class="author-avatar u-photo" alt="Matthew Burke"><div class="author-info"><div class="author-name">
        <span class="p-name">Matthew Burke</span>
      </div><ul class="author-links"><li class="author-link">
            <a class="u-url" rel="me" href="https://instagram.com/yot_club_"><i class="fab fa-instagram fa-lg" title="Instagram"></i></a>
          </li><li class="author-link">
            <a class="u-url" rel="me" href="https://github.com/mwburke"><i class="fab fa-github-square fa-lg" title="GitHub"></i></a>
          </li></ul>

<span class="read-time">4 min read</span>

    <time class="page-date dt-published" datetime="2018-11-26T00:00:00-08:00"><a class="u-url" href="">November 26, 2018</a>
</time>

  </div>
</div>

        
  <h3 class="page-taxonomies-title">Categories</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a class="p-category" href="/categories/#data-science" title="Pages filed under Data Science">Data Science</a></li>
  </ul>


        
  <h3 class="page-taxonomies-title">Tags</h3>
  <ul class="page-taxonomies"><li class="page-taxonomy"><a href="/tags/#probability" title="Pages tagged probability" rel="tag">probability</a></li><li class="page-taxonomy"><a href="/tags/#ml" title="Pages tagged ml" rel="tag">ml</a></li>
  </ul>


      </div>

      <div class="page-content">
        <div class="e-content">
          <h1 id="predictions-as-confidence">Predictions As Confidence</h1>

<p>As you may already know, classification problems in machine learning commonly (though not always) use algorithms that output a <em>predicted probability</em> value that can be used to gauge confidence in how sure your model is that the input belongs to one particular class.</p>

<h1 id="setting-probability-thresholds">Setting Probability Thresholds</h1>

<p>In introductory ML courses, a default value of 0.50 is usually used as the prediction cutoff for making the decision to consider a binary classification output as either positive or negative class, but in industry, selecting the right cutoff threshold is critical to making good business decisions.</p>

<p>If the cost associated with false negatives is large, it may be optimal to use a lower probability decision threshold to capture more positive users at the expense of including more false positives, and vice versa, and the data scientist will work with the business units to balance this tradeoff in order to minimize cost or maximize the benefit. Ultimately, this causes the predictions to act more as a ranking system for applications that require binary classifications as the final output than actually leveraging the values themselves.</p>

<h1 id="interpretation-problems">Interpretation Problems</h1>

<h2 id="model-as-ranking">Model As Ranking</h2>

<p>This model-as-ranking system works fine in many situations, but what happens when your predicted probability does not actually represent the probability and the business unit consuming your predictions assumes that they are? An example of this might be the likelihood of conversion for a given user, which is then multiplied by potential LTV to prioritize leads for a sales organization based on expected ROI. If a model tends to over/underestimate probabilities at the lower/upper ends of the predicted probability spectrum respctively (as random forest models have been known to do), you can end up spending effort on individuals who are less worth the team’s time, wasting resources and potentially losing revenue.</p>

<p>Scikit-learn has a great overview on some common algorithms that result in biased predicted probabilities. I’ve taken the liberty of displaying the chart from that overview here. Visit <a href="https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html">this link</a> to get the full code used to generate the plot or just look at the documentation for the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html">sklearn.calibration.calibration_curve</a> function</p>

<p><img src="/images/calibration_curve_1.png" alt="https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html" /></p>

<h2 id="parallel-model-consumption">Parallel Model Consumption</h2>

<p>Additionally, models can be used in conjunction with one another to provide targets in context. Going back to our expected LTV example, a business may have separate conversion likelihood models for different segments of their customer population, with every user being assigned a conversion probability from a single model. If not all models produce well-calibrated predicted probabilities, one could end up dominating the others while still having good metrics when considered individually.</p>

<h3 id="auroc-can-be-misleading">AUROC Can Be Misleading</h3>

<p>One common performance metric that is used to measure the effectiveness of the model across the range of predicted probabilities is the area under the receiving operating characteristic (ROC) curve. In case you aren’t familiar with the ROC curve, it is a plot of the model’s true positive rate vs the false positive rate as the probability is varied from 0 to 1, and as such, it is considered more of a robust metric than accuracy alone in cases where classes are imbalanced or the cost of true/false positives are unknown as of yet.</p>

<p>While it is a good metric, it is <strong>not</strong> sensitive to the absolute value of the predicted probabilities, only the performance at every probability point. If all of the predicted probabilities are multiplied by a constant, the value of the AUROC does not change, which may mislead the modeler into believing their probabilities are good to use, while in fact, they are consistently over/underestimating the results.</p>

<p>For example, the three predicted probability density distributions below are just scaled versions of the output from the same model. Their distributions are obviously very different from one another, but because they are scaled by a constant, they all have an equivalent AUROC score.</p>

<p><img src="/images/pred_probs_scaled.png" alt="" /></p>

<h1 id="validation-with-additional-scoring-methods">Validation with Additional Scoring Methods</h1>

<p>As with most modeling, it’s impossible to represent overall performance with a single number, and if you have concerns about validating probability calibration, it seems wise to include additional scores alongside AUROC that are more representative of actual differences in calibration such as log loss or the brier score.</p>

<p>Log loss is a common loss function, but <a href="https://en.wikipedia.org/wiki/Brier_score">brier score</a> was new to me, and according to wikipedia “can be thought of as… a measure of the ‘calibration’ of a set of probabilistic predictions”. It essentially is the average squared difference between the probability that was forecast and the actual outcome of the event. This makes its interpretation analogous to the RMSE for regression problems, and does take into account the scale of the predictions.</p>

<h1 id="sampling-bias">Sampling Bias</h1>

<p>Many problems have imbalanced datasets in terms of the target variable with a significant portion of the records belonging to one class. Various techniques have been developed to counteract these problem, including oversampling the minority class, downsampling the majority class and generating synthetic samples from the minority class to closer achieve class number parity. However, these techniques can result in increased AUROC scores while biasing the predicted probabilities to be less calibrated to actual.</p>

<p>Here is an example of how a generally well calibrated classifier (Logistic Regression) can be biased depending upon the ratio of the positive to negative class in the training dataset:</p>

<p><img src="/images/calibration_curve_2.png" alt="" /></p>

<h2 id="further-research">Further Research</h2>

<p>Scikit-learn has implemented the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html">CalibratedClassifierCV</a> class to adjust your classifiers to be more calibrated either during training, or to adjust the predictions by calibrating the classifier post-training.</p>

<p>It has two options for doing so:</p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Platt_scaling">Platt Scaling</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Isotonic_regression">Isotonic Regression</a></li>
</ul>

        </div>

        

        

        <nav class="page-pagination" role="navigation">
  
    <a class="page-previous" href="/generative%20art/2018/07/09/generative-art-p5js.html">
      <h4 class="page-pagination-label">Previous</h4>
      <span class="page-pagination-title">
        <i class="fas fa-arrow-left"></i> My Intro to Generative Art

      </span>
    </a>
  

  
    <a class="page-next" href="/data%20visualization/2018/12/04/idyll-pumpkin-taste-test.html">
      <h4 class="page-pagination-label">Next</h4>
      <span class="page-pagination-title">
        Introduction to Idyll
 <i class="fas fa-arrow-right"></i>
      </span>
    </a>
  
</nav>

      </div>
    </div>
  </article>
</main>


    <footer id="footer" class="site-footer">
  <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
<div class="social-icons"><a class="social-icon" href="https://instagram.com/yot_club_"><i class="fab fa-instagram fa-2x" title="Instagram"></i></a><a class="social-icon" href="https://github.com/mwburke"><i class="fab fa-github-square fa-2x" title="GitHub"></i></a><a class="social-icon" href="/atom.xml"><i class="fas fa-rss-square fa-2x" title="Feed"></i></a></div><div class="copyright">
    
      <p>&copy; 2023 Matthew Burke's Blog. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/mmistakes/so-simple-theme" rel="nofollow">So Simple</a>.</p>
    
  </div>
</footer>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>


<!-- MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>

</html>
